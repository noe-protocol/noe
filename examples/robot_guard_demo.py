"""
robot_guard_demo.py - Unified Robot Safety Guard Simulation

Demonstrates Noe's deterministic safety verification in a simulated robot control loop.

Validates:
- Strong Kleene (K3) logic: undefined propagation prevents unsafe assumptions
- π_safe projection: C_rich → C_safe filtering with epistemic thresholds
- Strict validation: ERR_STALE_CONTEXT, ERR_CONTEXT_CONFLICT detection
- Provenance: SHA-256 action hashes for audit trails
- Multi-scenario coverage: Safe execution, stale context, recovery, conflict, missing data

Context Pipeline:
    C_root + C_domain + C_local → C_rich → π_safe → C_safe → Noe Runtime

All evaluation uses C_safe only (post-projection context).
"""

import os
import json
import time
import shutil
from datetime import datetime
from copy import deepcopy

# Adjust path to import noe from root if needed
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from noe.noe_runtime import NoeRuntime
from noe.context_manager import ContextManager

# ==========================================
# CONFIGURATION
# ==========================================
LOG_DIR = "guard_logs"
LOG_FILE = os.path.join(LOG_DIR, "decision_log.jsonl")
MAX_SKEW_MS = 100.0  # Strict freshness requirement

# ==========================================
# SCENARIOS
# ==========================================
# We simulate a "Tick" loop.
# Scenarios define the state of the world (Context) and the Agent's Request (Chain)
SCENARIOS = [
    {
        "tick": 1,
        "name": "SAFE_EXECUTION",
        "desc": "Context is fresh, Agent asks for supported action.",
        "drift_ms": 10,   # Fresh
        "chain": "men @safe_zone",
        "inject_fault": None
    },
    {
        "tick": 2,
        "name": "STALE_CONTEXT",
        "desc": "Context drift exceeds max_skew_ms. Validator assumes unsafe.",
        "drift_ms": 150,  # > MAX_SKEW_MS
        "chain": "men @safe_zone",
        "inject_fault": None
    },
    {
        "tick": 3,
        "name": "RECOVERY_TICK",
        "desc": "Sensors refresh context. System recovers from staleness.",
        "drift_ms": 5,    # Fresh again
        "chain": "men @safe_zone",
        "inject_fault": None
    },
    {
        "tick": 4,
        "name": "EPISTEMIC_CONFLICT",
        "desc": "Agent claims to know something not in context knowledge base.",
        "drift_ms": 10,
        "chain": "shi @hidden_danger",  # "I know hidden_danger" (but context doesn't have it)
        "inject_fault": None
    },
    {
        "tick": 5,
        "name": "MISSING_SHARD",
        "desc": "Context is missing the 'spatial' shard required for the action.",
        "drift_ms": 10,
        "chain": "men @safe_zone",
        "inject_fault": "remove_spatial"
    },
    {
        "tick": 6,
        "name": "GUARDED_ACTION_PASS",
        "desc": "Epistemic guard allows action when knowledge condition is met.",
        "drift_ms": 10,
        "chain": "shi @human_clear khi sek mek @move_to_zone1 sek nek",
        "inject_fault": None
    },
    {
        "tick": 7,
        "name": "GUARDED_ACTION_BLOCK",
        "desc": "Epistemic guard blocks action when knowledge is false.",
        "drift_ms": 10,
        "chain": "shi @human_clear khi sek mek @move_to_zone1 sek nek",
        "inject_fault": "block_human_clear"
    }
]

# Base Context Template (v1.0 Layered Structure)
BASE_CONTEXT = {
    "root": {
        "literals": {
            "@safe_zone": True,
            "@hidden_danger": False,  # In literals, but NOT known in modal.knowledge
            "@human_clear": True,     # For guarded action test
            "@move_to_zone1": {"id": "zone1_target", "type": "location"}
        },
        "entities": {
            "robot": {"type": "agent"},
            "obstacle": {"type": "obstacle"}
        },
        "spatial": {
            "unit": "meters", 
            "thresholds": {"near": 1.0, "far": 5.0},
            "orientation": {"target": 0.0, "tolerance": 0.1}
        },
        "temporal": {
            "now": 0.0, # Will be updated
            "max_skew_ms": MAX_SKEW_MS
        },
        "modal": {
            "knowledge": {
                "@safe_zone": True,  # Robot knows safe_zone
                "@human_clear": True  # Robot knows human is clear (for guarded action)
            },
            "belief": {},
            "certainty": {}
        },
        "axioms": {
            "value_system": {
                "accepted": [],
                "rejected": []
            }
        },
        "rel": {},
        "demonstratives": {},
        "delivery": {"status": {}},
        "audit": {"files": {}, "logs": []}
    },
    "domain": {},
    "local": {}
}

def setup_logs():
    if os.path.exists(LOG_DIR):
        shutil.rmtree(LOG_DIR)
    os.makedirs(LOG_DIR)
    print(f"[*] Initialized log directory: {LOG_DIR}")

def get_simulated_context(tick_cfg, base_start_time):
    """
    Constructs a context object simulating sensor data at a specific time.
    """
    ctx = deepcopy(BASE_CONTEXT)
    
    # Simulate time
    # "now" in context is the sensor timestamp.
    # We pretend current wall time is (sensor_ts + drift)
    
    sensor_ts = base_start_time + (tick_cfg["tick"] * 1000) # 1 sec per tick
    
    # Update context timestamp in root layer
    ctx["root"]["temporal"]["now"] = sensor_ts
    
    # Local timestamp (simulating the validator's clock reading context)
    # If we want drift_ms, we set local timestamp to sensor_ts + drift
    current_time_ms = sensor_ts + tick_cfg["drift_ms"]
    
    ctx["local"] = {
        "timestamp": current_time_ms,
        "agent_id": "robot_01"
    }

    # Inject Faults
    if tick_cfg["inject_fault"] == "remove_spatial":
        del ctx["root"]["spatial"]
    
    if tick_cfg["inject_fault"] == "block_human_clear":
        # Set knowledge to False to block the guarded action
        ctx["root"]["modal"]["knowledge"]["@human_clear"] = False
        
    return ctx

def write_audit_record(record):
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(record) + "\n")

def run_robot_loop():
    commit_hash = os.getenv("GIT_COMMIT", "dirty")
    registry_hash = "a1b2c3d4e5f6..."
    
    print(f"[*] Starting Robot Guard Simulation")
    print(f"    Commit:   {commit_hash}")
    print(f"    Registry: {registry_hash}")
    print(f"    Strict:   True")
    print(f"    MaxSkew:  {MAX_SKEW_MS}ms")
    
    base_time = 1700000000000.0 # Arbitrary epoch ms
    actual_verdicts = {}
    
    for scenario in SCENARIOS:
        tick = scenario["tick"]
        print(f"\n--- TICK {tick}: {scenario['name']} ---")
        print(f"    Desc: {scenario['desc']}")
        print(f"    Chain: {scenario['chain']}")
        
        # 1. Update Context (Simulate Sensor Reading)
        ctx_data = get_simulated_context(scenario, base_time)
        
        # 2. Noe Validator Step
        start_ns = time.monotonic_ns()
        
        from noe.context_manager import ContextStaleError, BadContextError
        try:
            cm = ContextManager(
                root=ctx_data.get("root", {}),
                domain=ctx_data.get("domain", {}),
                staleness_ms=int(ctx_data.get("root", {}).get("temporal", {}).get("max_skew_ms", MAX_SKEW_MS)),
                # time_fn is defined as the current monotonic tick time simulating the validator's real wall clock
                # ContextManager converts this internally via int(time_fn() * 1000)
                time_fn=lambda: (base_time + scenario["drift_ms"]) / 1000.0
            )
            # Ensure the last update reflects the snapshot's base_time, not the drifted eval time
            cm.update_local(ctx_data.get("local", {}))
            cm._last_local_update_ms = int(base_time)
            runtime = NoeRuntime(context_manager=cm, strict_mode=True)
            
            # Explicitly validate the snapshot for missing shards (like missing spatial)
            from noe.noe_validator import validate_context_strict
            snap = cm.snapshot()
            if snap.is_stale:
                raise ContextStaleError("Context skewed beyond max_skew_ms")
            is_valid, err_msg = validate_context_strict(snap.structured)
            if not is_valid:
                raise BadContextError(err_msg)
            
            # We use strict mode for the robot guard
            # All freshness and context validation happens internally within evaluate()
            result_obj = runtime.evaluate(scenario["chain"])
        except (ContextStaleError, BadContextError) as e:
            class MockResult: pass
            result_obj = MockResult()
            result_obj.domain = "error"
            result_obj.error = "ERR_CONTEXT_STALE" if isinstance(e, ContextStaleError) else "ERR_BAD_CONTEXT"
            result_obj.value = str(e)
            result_obj.context_hash = "N/A"
        
        duration_ns = time.monotonic_ns() - start_ns
        
        # 3. Analyze Verdict
        # Actions can be returned as domain="action" OR domain="list" (for sek blocks)
        verdict = "ALLOWED" if result_obj.domain in ["action", "list"] else "BLOCKED"
        reason_code = getattr(result_obj, "error", None)
        reason_msg = str(getattr(result_obj, "value", "Unknown Failure")) if result_obj.domain == "error" else None
        
        # Extract strict codes from message if passed through value or error (e.g. 'ERR_EPISTEMIC_MISMATCH: msg')
        if reason_code and ":" in reason_code and reason_code.split(":")[0].startswith("ERR_"):
            parts = reason_code.split(":", 1)
            reason_code = parts[0].strip()
            if not reason_msg or reason_msg == "Unknown Failure" or reason_msg == getattr(result_obj, "error", None):
                reason_msg = parts[1].strip()
                
        if result_obj.domain == "error" and not reason_code:
            if reason_msg and ":" in reason_msg and reason_msg.split(":")[0].startswith("ERR_"):
                parts = reason_msg.split(":", 1)
                reason_code = parts[0].strip()
                reason_msg = parts[1].strip()
            else:
                reason_code = "error"
                
        if not reason_code:
            reason_code = result_obj.domain
        
        # Refine Epistemic/Missing Errors per Spec
        missing_shards = []
        epistemic_evidence = []
        
        if tick == 4: # EPISTEMIC_CONFLICT
             # Error code expected from strict validation of shi check failing against C_safe
             # Our NoeRuntime already transforms this into ERR_EPISTEMIC_MISMATCH under the hood 
             # if the domain resolves to undefined/error. Just extract it naturally.
             if reason_code == "error":
                 # Fallback if details are missing
                 if not reason_msg: reason_msg = "Claim 'shi @hidden_danger' not supported by C.modal.knowledge"
                 epistemic_evidence = ["MISSING: @hidden_danger"]

        if tick == 5: # MISSING_SHARD
             missing_shards = ["spatial"]

        # Colorized Output
        color = "\033[92m" if verdict == "ALLOWED" else "\033[91m" # Green/Red
        reset = "\033[0m"
        
        print(f"    Verdict: {color}{verdict}{reset} ({reason_code})")
        if verdict == "ALLOWED":
            # Check scenario to determine action type
            if tick in [6, 7]:  # Guarded action scenarios
                print(f"    Action:  MOVE_TO_ZONE1 (guarded)")
                # Extract action hash from result
                action_data = result_obj.value
                if isinstance(action_data, list) and len(action_data) > 0:
                    action_hash = action_data[0].get("action_hash", "N/A")
                    event_hash = action_data[0].get("event_hash", "N/A")
                    print(f"    ├─ action_hash: {action_hash[:16]}...")
                    print(f"    └─ provenance: PRESENT")
            else:
                print(f"    Action:  MOVE_TO_SAFE_ZONE")
        else:
            print(f"    Reason:  {reason_msg}")
            if tick in [6, 7]:  # Guarded action scenarios
                # Show that blocked actions have NO hashes
                print(f"    ├─ action_hash: null")
                print(f"    ├─ provenance_hash: null")
                print(f"    └─ Proof: Blocked ≠ Executed")
            if missing_shards:
                print(f"    Missing: {missing_shards}")

        # 4. Generate Audit Artifact
        audit_record = {
            "tick": tick,
            "scenario": scenario["name"],
            "wall_time_iso": datetime.utcnow().isoformat() + "Z",
            "monotonic_time_ns": time.monotonic_ns(),
            "execution_duration_ns": duration_ns,
            "chain_text": scenario["chain"],
            "verdict": verdict,
            "result_domain": result_obj.domain,
            "reason_code": reason_code,
            "error_details": reason_msg if verdict == "BLOCKED" else None,
            "action": ("MOVE_TO_ZONE1" if tick in [6, 7] else "MOVE_TO_SAFE_ZONE") if verdict == "ALLOWED" else None,
            "hashes": {
                "registry_hash": "a1b2c3d4e5f6...", # Mocked for demo
                "commit_hash": os.getenv("GIT_COMMIT", "dirty"),
                "chain_hash": hash(scenario["chain"]), 
                "context_hash": result_obj.context_hash,
                "action_hash": (
                    result_obj.value[0].get("action_hash") 
                    if verdict == "ALLOWED" and isinstance(result_obj.value, list) and len(result_obj.value) > 0
                    else result_obj.value.get("action_hash") if verdict == "ALLOWED" and isinstance(result_obj.value, dict)
                    else None
                )
            },
            "system_state": {
                "drift_ms": scenario["drift_ms"],
                "max_skew_ms": MAX_SKEW_MS,
                "shards_present": list(ctx_data.keys()),
                "missing_shards": missing_shards
            },
            "epistemic_check": {
                "claims": [t for t in scenario["chain"].split() if t.startswith("shi")],
                "missing_evidence": epistemic_evidence
            }
        }
        
        # Record for golden validation
        actual_verdicts[tick] = f"{verdict}:{reason_code}" if verdict == "BLOCKED" else "ALLOWED"
        
        write_audit_record(audit_record)

    print(f"\n[*] Simulation Complete. Audit log written to {LOG_FILE}")
    
    # 5. Golden Verdict Vector Assertion (Anti-Regression Anchor)
    # This prevents the simulator from silently decaying in CI if rules relax
    golden_verdicts = {
        1: "ALLOWED",
        2: "BLOCKED:ERR_CONTEXT_STALE",
        3: "ALLOWED",
        4: "BLOCKED:ERR_EPISTEMIC_MISMATCH",
        5: "BLOCKED:ERR_BAD_CONTEXT",
        6: "ALLOWED",
        7: "BLOCKED:undefined"
    }
    print("\n[*] Validating against golden verdict vector:")
    passed = True
    for t_id, expected in golden_verdicts.items():
        actual = actual_verdicts.get(t_id)
        if actual == expected:
            print(f"    Tick {t_id}: MATCH ({actual})")
        else:
            print(f"    Tick {t_id}: REGRESSION! Expected '{expected}', got '{actual}'")
            passed = False
    
    if not passed:
        import sys
        sys.exit(1)
    else:
        print("    All ticks matched golden vector.")

if __name__ == "__main__":
    setup_logs()
    run_robot_loop()
